---
title: "Tree models"
author: "James Scott"
date: "ECO 395M: Data Mining and Statistical Learning"
output: beamer_presentation
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_chunk$set(cache=TRUE)

prune_1se = function(my_tree) {
  require(rpart)
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

```

## Tree models

```{r, out.width = "300px", fig.align='center', echo=FALSE}
knitr::include_graphics("fig/umbrella_tree.png")
```

Trees involve simple mini-decisions that combine to make a choice or prediction.  

Each decision is a _node_; the final choice or prediction is a _leaf node._  


## Tree models

You can think of a tree as a form of regression model:   

- inputs $x$: forecast, current conditions  
- output $y$: need for an umbrella  

Based on previous data, the  the goal is to specify branches/choices that lead to good predictions in new scenarios.

In other words, you want to estimate a __Tree Model__. Instead of linear coefficients, we need to find 'decision nodes': binary splitting rules defined by the x features.   


## Tree models

Tree models come in two flavors.  

- Classification tree: the leaf nodes are predicted class labels/probabilities for a categorical outcome.  (__"The predicted probability of rain is 90%, so take an umbrella."__)
- Regression tree: the leaf nodes specify E(y | x) for a numerical outcome.  (__"The predicted amount of rain is 2 cm, so take an umbrella."__)     

The basic idea is the same for both; just a few of the details change.  

## Tree models

The goal of tree modeling: specify the sequence of mini-decisions that get you to the leaves.   

- How many?  
- What decisions? (Which features, which values of the features?)  
- What order?    

The space of possible trees is _huge_.  

We'll talk about fitting the tree later; for now, let's see some examples to build our intuition.   




## A classification tree  

_Classification trees_ are for categorical outcomes (with binary outcomes as a special case).  

Let's see an example trained on data from the Titanic:  

- x: a passenger's sex, age, class of travel
- y: whether the passenger survived the sinking of the ship  

Goal: estimate $P(y \mid x)$


## A classification tree  

:::::: {.columns}

::: {.column width="50%"}
```{r, echo=FALSE, fig.align='left', fig.width=2.5, fig.asp = 1, message=FALSE}
library(tidyverse)
library(ggplot2)
library(tree)
library(rpart)
library(rpart.plot)
titanic = read.csv('titanic.csv', stringsAsFactors = TRUE)

temp_tree = rpart(survived ~ sex + age + passengerClass, data=titanic)
rpart.plot(temp_tree, type=4, extra=4)
```

:::

::: {.column width="50%"}
- Each split involves a yes/no question about a single variable.  
- For numerical features (age), the yes/no question is whether $x$ exceeds some threshold $t$.  
- You might see/hear this called a "dendrogram," which is just a fancy Latin word for "tree picture."  
:::

::::::


## A classification tree  

:::::: {.columns}

::: {.column width="50%"}
```{r, echo=FALSE, fig.align='left', fig.width=2.5, fig.asp = 1}
rpart.plot(temp_tree, type=4, extra=4)
```
:::

::: {.column width="50%"}
- At each leaf node, we see fitted class probabilities.  These come from the training data.   
- To make a prediction, you "drop" your x down the tree, answering each yes/no question in turn.  
- Notice the interactions!  E.g. the questions we ask about age at later splits depend on which branch we're on.  
:::

::::::




## A classification tree  


:::::: {.columns}

::: {.column width="50%"}
```{r, echo=FALSE, fig.align='left', fig.width=2.5, fig.asp = 1}
rpart.plot(temp_tree, type=4, extra=1)
```
:::

::: {.column width="50%"}
- It's easier to see where the fitted probabilities come from if we show the number of observations per class in each leaf node.  
- Let's reason through two quick examples.   
    - x = {male, 3rd, 5 years}  
    - x = {female, 1st, 25 years}.
:::

::::::



## A regression tree  


_Regression trees_ are for numerical (as opposed to categorical) outcomes.

Let's see one on a familiar data set:   

- y = peak power consumption in the ERCOT coast region  
- x = temperature at Houston's Hobby airport in degrees C (so all splits are of the form $\mathrm{temp} < t$ for some threshold $t$). 

Goal: estimate $E(y \mid x)$


## A regression tree  


```{r, echo=FALSE, fig.align='center', fig.width=5, fig.asp = 0.6}
load_tree = read.csv('load_tree.csv')

# fit a big tree
load.tree = rpart(COAST~temp, data=load_tree,
                  control = rpart.control(cp = 0.002))
rpart.plot(load.tree, digits=-5, type=4, extra=1)
```

Now the leaf nodes show E(y | x), estimated by the average response (y) for the x's that "land" in that leaf.  


## A regression tree  


```{r, echo=FALSE, fig.align='center', fig.width=5, fig.asp = 0.6}
rpart.plot(load.tree, digits=-5, type=4, extra=1)
```

__What does $f(x) = E(y \mid x)$ look like as a function of x?__  


## A regression tree  


```{r, echo=FALSE, fig.align='center', fig.width=5, fig.asp = 0.6}
rpart.plot(load.tree, digits=-5, type=4, extra=1)
```

__Hint:__ the tree partitions the x space into disjoint regions.  Within each region, E(y | x) is constant.


## A regression tree


```{r, echo=FALSE, fig.align='center', fig.width=4, fig.asp = 0.6, message=FALSE}
load_tree = load_tree %>%
  mutate(COAST_pred = predict(load.tree)) %>%
  arrange(temp)

ggplot(mosaic::sample(load_tree, 2000)) + 
  geom_point(aes(x=temp, y=COAST), alpha=0.1) + 
  geom_step(aes(x=temp, y=COAST_pred), color='red', size=2)
```

This is the fitted regression function $f(x) \approx E(y \mid x)$.  It's a __step function__ (always the case with tree models).  



## A regression tree (two x's)


What if we consider the same problem, but with an additional feature?  

- $x_1$ = temperature at Houston's Hobby airport in degrees C  
- $x_2$ = dewpoint at Houston's Hobby airport in degrees C  
- y = peak power consumption in the ERCOT coast region  

The linear model equivalent would look like 
$$
E(y \mid x) = \beta_0 + \beta_1 x_1 + \beta_2 x_2
$$

What does the tree look like?   


## A regression tree (two x's)


```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6}
# fit a big tree
load.tree2 = rpart(COAST~temp + dewpoint, data=load_tree,
                  control = rpart.control(cp = 0.0015))
rpart.plot(load.tree2, digits=-5, type=4, extra=1)
```

Now each mini-decision can split on temperature or dewpoint (but not both).     


## A regression tree (two x's)

```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6, message=FALSE}
load_tree = load_tree %>%
  mutate(COAST_pred2 = predict(load.tree2)) %>%
  arrange(temp)

mysamp = mosaic::sample(load_tree, 5000)

ggplot(mysamp) + 
  geom_point(aes(x=temp, y=dewpoint, color=COAST_pred2)) + 
  scale_color_continuous(type = "viridis")
```

The tree partitions (x1, x2) space into rectangles.  Within each rectangle, the fitted value E(y | x1, x2) is constant.  

## A regression tree (two x's)


```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6, message=FALSE}
ggplot(mysamp) + 
  geom_point(aes(x=temp, y=dewpoint, color=COAST_pred2)) + 
  scale_color_continuous(type = "viridis")
```

The resulting fit is a step function in the 2D plane.  The regions of constant color show the steps.  


## A regression tree (two x's)


```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6, message=FALSE}
ggplot(mysamp) + 
  geom_point(aes(x=temp, y=dewpoint, color=COAST_pred2)) + 
  scale_color_continuous(type = "viridis")
```

Key point: __notice the interaction!__  Do you see it?  


## Trees: interactions

Trees provide automatic interaction detection (AID).  For example:  

- Effect of age on survival different for male and female passengers.  
- Effect of dewpoint on power consumption is different at low vs. high temperatures.


AID was an original motivation for building decision trees. (Older algorithms have it in their name: CHAID, US-AID, ...)

This is pretty powerful technology:

- automatic adaptation to nonlinearity and interaction, without having to specify it in advance!  (compare with `lm`...)
- Moreover, nonconstant variance is no problem.  


## Trees: a summary  

- Trees use recursive binary splits to partition the feature space.
- Each binary split is a rule that sends x left or right.
- For numeric x, the decision rule is of the form if x < c.
- For categorical x, the rule lists the set of categories sent left.  
- The set of bottom nodes (or leaves) give a partition of the x space.  
- To predict, we drop x down the tree until it lands in a leaf node.  
   - For numeric y, we predict using that leaf's average y value from the training data.  
   - For categorical y, predict using that leaf's  category proportions from the training data.   


## Tree models: pros and cons


Pros:

- Flexible fitters that can automatically detect nonlinearities and interactions.  
- Invariant to transformations of the x variables.  
- Handles categorical and numeric variables easily.  
- Fast to fit.  
- Interpretable (when small).  

## Tree models: pros and cons


Cons:  

- Inherently non-smooth (step functions).  
- Don't scale to very large feature sets.  
    - Trees can bog down with hundreds or thousands of features.  
    - But can often fix this with dimension-reduction techniques.  
- Not the best at out-of-sample performance.  
    - Generally must be deep to predict well (and thus less interpretable).  
    - But still not bad at prediction!  
    - And by _ensembling_, or averaging multiple trees, we can get excellent off-the-shelf predictions.  



## Fitting trees


As usual, we'll maximize data log-likelihood (minimize deviance)---here by fiddling with the tree's decision nodes.

- How many?
- What order?
- What feature and how to split that feature?

Two common loss functions:   

- Regression deviance: $\sum_{i=1}^n (y_i - \hat{y}_i(x_i))^2$    
- Classification deviance: $-\sum_{i=1}^n \log \hat{p}_{y_i}(x_i)$    

Instead of being based on $x \cdot \beta$, predicted $\hat{y}$ and $\hat{p}$ are functions of $x$ passed through the tree's decision nodes, just like we've seen.  


## Fitting trees


How do we do the minimization?  

Now we have a problem.

- While trees are simple in some sense, once we view them as variables in an optimization, they are large and complex.     
- A key to tree modeling is the success of the following algorithm for fitting trees to training data: __grow big, prune back.__   
- This algorithm is _greedy_ and _recursive_.  Let's unpack those terms.  


## Grow big


Use a greedy, recursive forward search to build a big tree, starting with all data in a single node (one leaf).  

For each leaf node, _get greedy_:  

 1. Search over all possible splitting rules to find the single split that gives the biggest decrease in loss (increase in fit).   This can be done very quickly.  
 2. Using this optimal rule, split this "parent" into two "children".    

Then _repeat recursively_, treating each child as a new parent.  You typically stop splitting and growing when the size of the leaf node hits some minimum threshold (e.g., 10 obs. per leaf).   


## Grow big


```{r, fig.width=4.5, out.width=300, fig.align='center', echo=FALSE}
knitr::include_graphics("fig/tree_recursive.png")
```

The key word is _recursive_---like how trees grow in the real world!  


## Prune back


Given a current tree with D leaf nodes:     

  1. Examine every pair of "sibling" leaf nodes (i.e. leaf nodes of the tree having the same parent) and check the increase in loss (decease in fit) from "pruning" that split.
  2.  Prune the "least useful" split, i.e. the prune that yields the smallest increase in loss (decrease in fit).  
  
Repeat recursively on the newly pruned tree having D-1 leaf nodes.  Stop when you've pruned all the way down to a single node.    

__Let's see this process on the board.__


## Prune back




## Prune back


Why grow, then prune?  To __generate a sequence of trees__ of sizes $D, D-1, D-2, \ldots, 2, 1$, each tree locally optimal for its size.   

A good analogy is the lasso solution path:  

- The big trees fit best, but have lots of splits and fewer data points in each leaf node.  (Lower bias, higher variance).   
- The small trees fit less well, but are simpler and have more data points in each leaf node.  (Higher bias, lower variance.)

Grow-then-prune yields candidate trees that (hopefully) span the bias-variance trade-off.  We can then use cross-validation to choose.    


## CART

This basic fitting algorithm is called CART, for "classification and regression trees."

CART is also sometimes called "recursive partitioning" and this is reflected in the R syntax:  

\footnotesize
```
load.tree = rpart(COAST~temp + dewpoint, data=load,
                  control = rpart.control(cp = 0.002, minsplit=30))
```

\normalsize

`control` gives the "stopping points" for controlling tree growth.  You'll often want to change these from their defaults.  Here we split a node only if:  

- it has at least 30 observations...
- AND if the split improves the deviance by a factor of 0.002 (0.2%).    


## CART

So to recap, CART:  

- Grow the tree greedily and recursively to make deviance as small as possible.  
- Stop growing when you hit your minimum size or complexity stopping points.    
- Prune back from there to generate candidate trees.  
- Choose by cross validation (min or 1SE).  

Let's go see some examples in `tree_examples.R`.  


## CART: what we learned

From our examples, we learned a few things:  

1. As tree complexity increases, CV error generally goes down quickly, levels off, and goes back up really s-l-o-w-l-y.  
2. A sensible way to pick a tree is to use the "1SE rule": 
    - Choose the smallest tree whose cross-validated error is within one standard error of the minimum.   
    - This gives a simpler model whose performance is not discernibly different from the best performer.   
3. Trees that perform well tend to be pretty deep.  (This is often true even on simple problems---but still worth trying, you might be surprised.)  



## CART: what we learned

This last point---that good trees tend to be deep trees---is especially concerning.  

In deep trees, at least some (and perhaps most) of the leaf nodes have very few observations in them.  

- This deep structure makes trees inherently prone to overfitting.  
- They tend to find mini-decisions that memorize random noise, in addition to the underlying signal.    

Deep trees also ruin interpretability: 

- a small tree with a handful of mini-decisions can be interpreted by a person...
- but probably not hundreds or thousands of mini-decisions in a deep tree.   


## A toy example

```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6, message=FALSE}
x  = runif(250, 0, 10)
y = x + rnorm(length(x), 0, 0.2*sd(x))
fake_data = data.frame(x, y)
ggplot(fake_data) + 
  geom_point(aes(x,y), alpha=0.2)
```
True model: $y = x + e, \; e \sim N(0, 1))$


## A toy example

```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6, message=FALSE}
example_tree = rpart(y ~ x, data=fake_data, 
                     control = rpart.control(cp = 0.0005, minsplit=2))
#example_tree = prune_1se(example_tree)
rpart.plot(example_tree)
fake_data = mutate(fake_data, y_hat = predict(example_tree))
```
A single tree fit using the 1SE rule.  

## A toy example

```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6, message=FALSE}
ggplot(arrange(fake_data, x)) + 
  geom_point(aes(x,y), alpha=0.2) + 
  geom_step(aes(x, y_hat), color='red', size=1)
```
A single tree fit using the 1SE rule.  


## Bagging: bootstrap aggregating

__Bagging__---or Bootstrap Aggregating:  

- involves averaging the predictions from multiple trees.  
- is a way to _reduce estimation variance without adding bias,_ thereby preventing trees from overfitting quite so badly.  
- doesn't address interpretability (but hey, deep trees are hard to interpret anyway!)   
\pause

Let's see _how_ bagging works, before we consider _why_ it works:  

- __Bootstrap__: resample the data with replacement $B$ times, to get $B$ "jittered" versions of your original data set (each size $n$).  
- __Fit__: for each bootstrapped data set, fit a deep tree by CART.  
- __Aggregate__: when you want to predict $y$ for some $x$, average the predictions from this "forest" of B trees.

Remember our basic intuition for why averaging works in the most basic problem of all: estimating a mean.    

$$
y_i = \mu + e_i
$$

- Think of $\mu$ as the signal and $e_i$ as the noise.  
- We take a bunch of IID samples $y_i$, $i = 1, \ldots, n$.  
- We estimate $\mu$ as $\hat\mu = (1/n) \sum_i x_i$.  
- When you take a bunch of samples and average them, the individual noise terms "wash out" in the averaging.  
- But $\mu$ is there in each sample and doesn't wash out.  


## Why on earth would this work?  

It's kind of the same in bagging, where we think $y_i  = f(x) + e_i$:  

- We take bootstrapped samples $b = 1, \ldots, B$ and build lots of big trees to give an estimate $\hat{f}^{(b)}(x)$.  
- We estimate $f(x)$ as the average  
$$
f(x) = \frac{1}{B} \sum_b \hat{f}^{(b)}(x)  
$$
- Wiggles in $f(x)$ that are real captured in most or all of the bootstrapped estimates.    
- Wiggles that are over-fit to a few data points, "by chance," are idiosyncratic to only a few estimates, and get "washed out" in the averaging.\footnote{\tiny The math is a bit more complicated than on the previous slide, because the bootstrapped estimates are correlated with each other.  But they're not perfectly correlated, which is the source of variance reduction.}    

Leo Breiman.  __Brilliant.__  


## Let's see it work.

```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6, message=FALSE}
ggplot(arrange(fake_data, x)) + 
  geom_point(aes(x,y), alpha=0.2) + 
  geom_step(aes(x, y_hat), color='red', size=1)
```
Original fit.

## Let's see it work.

```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6, message=FALSE}
boot_agg = NULL
x_grid = seq(0, 10, by=0.1)
for(b in 1:100) {
  this_boot = mosaic::resample(fake_data)
  boot_tree = rpart(y ~ x, data=this_boot, 
                     control = rpart.control(cp = 0.0002, minsplit=2))
  boot_tree = prune_1se(boot_tree) 
  this_pred = data.frame(b=b, x=x_grid, y_hat=predict(boot_tree, data.frame(x=x_grid)))
  boot_agg = rbind(boot_agg, this_pred)
}
boot_summ = boot_agg %>% group_by(x) %>% summarize(y_hat = mean(y_hat))

g1 = ggplot() + 
  geom_point(data=fake_data, aes(x,y), alpha=0.2) + 
  geom_step(data=boot_agg, aes(x,y_hat, group=b), col='red', size=0.5, alpha=0.05)
g1
```
Fits from 100 bootstrapped samples.

## Let's see it work.

```{r, echo=FALSE, fig.align='center', fig.width=4.5, fig.asp = 0.6, message=FALSE}
g1 + geom_line(data=boot_summ, aes(x,y_hat), col='blue', alpha=1, size=1)
```
Average of fits from 100 bootstrapped samples.    


## Bagging: a summary  

Fit trees to $B$ bootstrapped samples of the original data.  

- For numerical $y$, average the predictions.   
- For categorical $y$, let each tree vote, or average the trees' predicted class probabilities.  


You need $B$ large enough to enjoy the effect of averaging.  

- 100 is a decent starting point, 500 is better if your machine has the flops and memory.   
- It doesn't seem to hurt if you make $B$ even larger.  
- The only real cost of a large $B$ is computational time.   


## Bagging: a summary  

Bagging "works" because it usually gives you a smoother (lower-variance) fit than a single tree.  This is quite a general phenomenon:

- we can often improve a high-variance nonparametric regression model by bagging it.  This is the simplest form of "ensembling" or "model averaging"---super useful idea.   
- the corrollary is that a stable, low-variance estimator (e.g. a linear model) usually won't be improved by bagging.  
- trees are ideal candidates for bagging because they're high-variance, flexible fitters than are also quite fast to fit.  

The downside:  

- when we bag a model, any simple structure is lost.  
- this is because a bagged tree is no longer a tree, but a forest!  


## Random forests

A "random forest" starts from bagging...  

- We still take $B$ bootstrapped samples of the original data and fit a tree to each one.  
- We still average the predictions of the $B$ different trees.  

But it adds __more randomness.__  Within each bootstrapped sample:  

  - We don't search over __all__ the features in x when we do our greedy build of a big tree.  
  - Instead, we randomly choose a subset of $m < p$ features to use in building that tree.    

This does two things:

- it simplifies each tree, reducing its variance  
- it diversifies the $B$ trees, decorrelating their predictions  


## Random forests

Why would using fewer features in each tree actually help!?  

An analogy: evolutionary co-adaptation.  

- Co-adaptation occurs when two or more traits/genes undergo adaptation together as a group.  
- Co-adapted traits involve mutually adjusted changes.  Classic example: flowering plants and pollinating insects.  

A common way that trees overfit is by learning heavily "co-adapted" sets of features, i.e. deep interactions that explain noise.    

- Example: "No Democratic presidential incumbent without military service has ever beaten a challenger whose last name is worth more in Scrabble."  
- By forcing each tree to rely on only a few features, we force it to learn robust, generalizable relationships.  

## Random forests

In random forests, you must choose:  

- B: number of bootstrapped samples.  Use hundreds, or thousands if possible!  
- m: number of features to sample within each bootstrapped sample.  A common choice is $m \approx \sqrt{p}$.  

Some notes:  

- bagging is just random forests with $m=p$, but you'll typically see better performance with $m < p$.  
- there is no explicit regularization parameter, as in the lasso and single-tree models.  
- random forests might be the most popular "off the shelf" nonparametric regression technique.  They're effective, fast, and require little or no tuning via CV (the default settings perform well).  


## "Out-of-bag" error estimation 

With random forests, there's a nice built-in way to estimate the generalization error of the model.  

- In each bootstrap sample ("bag"), some of your original observations are "in the bag" (math says: about 2/3, on average).  
- The rest are "out of bag."  

By carefully keeping track of which trees use which observations, you can get "out-of-bag" predictions.  

- This is a decent way to estimate out-of-sample performance.  
- We typically use this to reassure ourselves we've used enough trees in the forest.  


## Random forests: example  

Let's go to `random_forest_example.R` to see random forests in action.  


## Boosting


